特点：强一致性的实现，本质上是**在CAP定理中选择了C（一致性）和P（分区容错性），而牺牲了A（可用性）**。
- 强一致性协议（如分布式锁、同步复制）通常要求所有副本都可用才能完成写入操作。
	- 如果一个副本因网络分区（网络中断）或机器宕机而无法联系，写入操作就会失败，导致服务不可用。
- 强一致性需要同步阻塞操作。
	- 例如，同步双写缓存需要等待缓存更新成功后才返回给用户；数据库主从同步需要等待所有从库确认后才认为写入成功。这增加了操作的响应时间（延迟）并限制了系统每秒能处理的请求数（吞吐量）。
- 实现方式：通过分布式共识、锁、事务协议或Quorum机制，将异步操作变为同步阻塞操作，确保数据的任何变更都能以原子的、线性的方式被所有参与者感知，从而在任何时刻都提供一个统一的数据视图。

| 实现方式         | 代表性技术栈                   | 适用场景              | 核心思想                  | 代价              |
| ------------ | ------------------------ | ----------------- | --------------------- | --------------- |
| **分布式共识协议**​ | etcd, Consul, ZooKeeper  | 元数据存储、配置管理、服务发现   | 大多数节点确认才成功            | 写延迟高，吞吐量相对较低    |
| **分布式锁**​    | 基于ZooKeeper/etcd/Redis的锁 | 秒杀库存、临界资源访问       | 串行化访问                 | 并发性能下降，死锁风险     |
| **两阶段提交**​   | JTA, Seata (AT/TCC模式)    | 跨数据库/服务的资金交易      | 事务原子性（All or Nothing） | 同步阻塞，性能差，复杂性高   |
| **线性一致性存储**​ | TiDB, Spanner, Aurora    | 金融核心系统、需要强一致的业务数据 | 读写Quorum机制            | 成本高，架构复杂，延迟高于单机 |
| **同步双写**​    | 应用代码+数据库事务               | 缓存与数据库的强一致        | 事务边界内完成多写             | 可用性降低，延迟增加      |
#### 1. 分布式共识协议
这是实现分布式系统强一致性的基石。它们确保在多个节点上对某个值达成一致，即使在节点故障或网络分区的情况下也能保证正确性。
- **代表性协议**：**Paxos**, **Raft**, ZAB (ZooKeeper Atomic Broadcast)。
- **工作原理**：这些协议要求一次写操作必须被**大多数（Quorum）**​ 节点确认后才被视为成功。这保证了即使少数节点宕机，系统仍能知道哪个值是“最新”的，并且不会回退到旧值。
- **技术栈与场景**：
    - **etcd / Consul**：使用Raft协议，用于服务发现、配置存储，需要强一致性保证。
    - **ZooKeeper**：使用ZAB协议，作为分布式锁、领导者选举的核心组件。
    - **分布式数据库的元数据管理**：如Google Spanner使用Paxos变种来管理数据的副本位置和事务状态。
#### 2. 分布式锁
在访问共享资源前，必须先获取锁，从而将并发操作串行化。
- **实现方式**：基于上述的共识系统（如ZooKeeper, etcd）或支持强一致性的Redis（Redlock算法）实现分布式锁。
- **场景**：
    - **防止超卖**：秒杀场景下，扣减库存前必须先获取该商品ID的锁，确保只有一个请求能进行“查询-判断-扣减”的流程。
    - **全局唯一序列号生成**：生成订单号等唯一ID时，需要锁来保证不重复。
- **代价**：锁的引入会显著降低系统的并发处理能力。
#### 3. 两阶段提交协议
用于在分布式事务中保证跨多个数据库或服务的操作的原子性（All or Nothing）。
- **工作原理**：
    1. **准备阶段**：事务协调者询问所有参与者（如数据库A、数据库B）：“能否提交？” 参与者锁定资源，准备事务日志，回答“是”或“否”。
    2. **提交阶段**：如果所有参与者都回答“是”，协调者发送“提交”命令，所有参与者正式提交；如果任何一个参与者回答“否”或超时，协调者发送“回滚”命令，所有参与者进行回滚。
- **技术栈**：Java EE的JTA规范，Seata等分布式事务框架。
- **代价**：同步阻塞问题严重，性能差，协调者单点故障风险高。
#### 4. 线性一致性/原子读写的存储系统
这类存储引擎在设计上就保证了强一致性。
- **实现方式**：
    - **主从同步复制**：所有写操作都必须同步到所有从库（或至少一个Quorum的从库）后才返回成功。读操作可以配置为只从主库读取，以避免主从延迟。
    - **Quorum读写机制**：基于NWR模型（N: 副本总数，W: 写成功副本数，R: 读成功副本数）。通过设置 `W + R > N`，可以保证读操作总能接触到至少一个包含最新数据的副本，从而实现强一致性。
- **技术栈与场景**：
    - **数据库**：**Google Spanner**, **TiDB**, **Amazon Aurora**（特定配置下）等新一代分布式关系型数据库。
    - **键值存储**：**ZooKeeper**, **etcd**，以及配置了 `W=majority, R=majority`的 **DynamoDB**​ 或 **Cosmos DB**。
#### 5. 同步双写
在应用层代码中，将更新数据库和更新缓存的操作放在同一个数据库事务中。
- **实现**：
    ```
    @Transactional
    public void updateProduct(Product product) {
        // 1. 更新数据库（主数据源）
        productDao.update(product);
        // 2. 在同一个事务中，同步更新缓存
        redisTemplate.opsForValue().set("product:" + product.getId(), product);
    }
    ```
- **代价**：
    - 缓存服务成为关键路径，如果缓存服务宕机，整个写操作都会失败，影响可用性。
    - 两个写操作叠加，延迟增加。