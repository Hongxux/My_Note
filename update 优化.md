- 问题分析与性能瓶颈：
	- 一次普通的`UPDATE`操作，数据库内部需要经历加锁、修改数据、生成重做日志（Redo Log）、写入二进制日志（Binlog）​ 等一系列步骤。
		- 直接问题：在并发环境下，当多个事务试图更新同一行数据时，会引发**激烈的锁竞争**
		- 衍生问题：获取锁失败的事务会进入等待状态，导致系统并发能力急剧下降
	- 对于海量数据的全表更新，即使没有锁竞争，也会因为需要修改大量数据页、产生巨大的Binlog（尤其是在`ROW`格式下）
		- 直接问题：占满**磁盘I/O**
		- 衍生问题：
			- 可能将缓冲池（Buffer Pool）中的热数据刷出
			- 从库需要逐条应用这些Binlog，极易导致主从延迟飙升，甚至从库崩溃
	- where的用的列没有索引或者索引失效，导致表锁
---
- 优化目标：最大限度地减少锁持有时间、降低I/O负载、将大操作化小
- SQL层优化：
	- 分批处理
		- 使用场景：需要更新千万级以上的数据
		- 解决措施：
			- **分批处理**：根据主键或唯一索引，每次更新一小批数据（如1000-5000行），批处理之间可以短暂睡眠，以释放CPU和I/O资源。
				- **高效分批写法**：使用`WHERE id > ? LIMIT ?`基于游标的方式，而不是`LIMIT offset, ?`进行深度分页，后者效率极低
			- **精准更新**：在`SET`子句中结合`CASE WHEN`条件，确保只更新需要改变的行，避免无谓的重写
	- 减小锁粒度，避免出现表锁
		- 建立索引
		- 避免索引失效
- 架构与设计层优化
	- 数据拆分：进行分库 
	- 变更数据捕获：
		- 使用场景：实时性要求不极高的统计类更新
		- 实现方式：
			- 将`UPDATE`操作转换为`INSERT`操作，记录下数据变更流水
			- 异步作业消费这些流水日志来计算最终状态