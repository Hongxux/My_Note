---
aliases:
  - Redis集群
---
- 需求背景：Redis主从和Redis哨兵可以解决高可用、高并发读的问题。但是依然有两个问题没有解决:
	- 海量数据存储问题
		- 单机内存有限，无法存储持续增长的海量数据（如大型社交媒体的用户数据）
		- 垂直扩展（Scale-up）升级硬件成本高且存在物理上限
	- 高并发写的问题
		- 单机CPU处理能力、网络I/O和连接数有上限，无法满足极高的读写并发需求
- 解决措施：分片集群架构：
	- 多主多从结构
		- ![[Pasted image 20251124100021.png]]
	- 核心思想：**去中心化**的架构和**分而治之**的策略，将多个Redis实例组织成一个统一的、可扩展的、高可用的数据库服务。
	- 多主多从的架构：
		- 客户端请求可以访问集群任意节点，最终都会被转发到正确节点(路由)
		- 集群有多个主节点
			- 每个主节点**负责一部分数据**：提高并发写的性能
			- 主节点相互ping：实现**自动故障转移**
		- 每个主节点可以挂载若干**从节点**
			- 用于高可用
			- 提高并发读的性能
	- 主节点之间通过ping检测彼此健康状态（类似于Redis哨兵机制）
		- 实现自动检测，故障转移和新主从状态的配置和通知
	- 优势
		- **分摊压力**：每个主节点负责一部分哈希槽，处理对应数据的读写请求。这样，写压力和存储压力就被分摊到了多个主节点上，实现了水平的写扩展
		- **[主从复制](app://obsidian.md/%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6.md)🔗**：每个主节点都可以配置一个或多个从节点。从节点通过异步复制保持与主节点的数据同步，主要提供**数据冗余**和**故障恢复**的能力。
		- **故障转移**：当某个主节点宕机时，集群会通过内置的共识算法，自动从其从节点中选举出一个新的主节点来接管槽位，继续提供服务，从而实现高可用
		- **容易扩容**：当数据量增加时，只需要新加节点D，重新分配部分哈希槽，集群即可扩容。

---
 **数据分片：哈希槽**
- 需求背景 ：
	- 简单哈希取模：节点数变动导致数据大规模重新映射
	- [[一致性哈希]]：节点分布不均易引起数据倾斜
- 核心思路：将数据与节点解耦，在此之间形成槽位-节点映射关系
- 核心优势：
	- 平滑扩容缩容
	- 可控的数据分布
- 实现基础
	- Redis集群预分了**16384个哈希槽**
	- 集群维护一份**槽位-节点映射表**：知道每个槽由哪个节点负责
- 数据分片的工作流程：
	- 每个Key中的**有效值**通过CRC16算法计算出一个哈希值，再对16384取模，确定其归属的槽位
		- key中包含“{}”，并且“{}”中至少包含1个字符，“{}”中的部分是有效部分
		- key中不包含“{}”，整个key都是有效部分
			例如：key是num，那么就根据num计算，如果是{ithm}num，则根据ithm计算。
		- 因此如果想要把同一类数据保存到一个Redis实例，从而同一有效值
			- 同一类数据保存到一个Redis实例的原因：避免多次使用重定向
			- 同一有效值的实现方式：可以使用{}括起来相同的值

**配置纪元：只增不减的整数值**
- 作用：避免出现配置冲突
- 责任：在槽位分配的争议解决中，`configEpoch`是最高法官，值越大优先级越高
- 具体机制：任何导致集群配置变更的操作都会使配置纪元递增

	  
**节点协调：[[Gossip]]协议，去中心化的通信机制**
- 常见情况的传播模式
	- 新节点加入
		1. **初始握手**：一个新节点（Node New）启动后，通过 `CLUSTER MEET`命令与集群中任意一个已知节点（Node A）建立联系。Node New 会向 Node A 发送一个 **MEET**​ 消息。
		2. **加入列表**：Node A 收到 MEET 消息后，会将 Node New 加入到本地维护的集群节点列表中，并回复一个 **PONG**​ 消息作为确认。
		3. **信息扩散**：
			- 扩散方式：Node A 会在其发送的 **PING**​ 消息体中，携带上 Node New 的信息（例如其IP、端口、节点ID等）。
			- 扩散效果：收到这条 PING 消息的其他节点（如 Node B、Node C）也会将 Node New 加入自己的节点列表
			- 扩散传播：并继续通过它们自己的 PING 消息将这一信息传播出去。
		4. **最终一致**：通过这样一轮轮的“八卦”，经过一小段时间，集群中的所有节点都会知道 Node New 的存在，从而达到最终一致的状态。
	- 节点下线
		1. **主观下线**：
			- 判断方式：一个节点A在 `cluster-node-timeout`时间内一直没有收到节点另一个节点B的PONG响应
			- 采取行动：
				1. 该节点A在本地将节点B标记为 `PFAIL`（可能失败）状态
				2.  在发送给其他节点的 **PING**​ 消息体中，会携带上节点 B 的 `PFAIL`状态
		2. **客观下线**
			- 判断方式：集群中**超过半数**的**主节点**都报告节点 B 处于 `PFAIL`状态
			- 采取行动：
				1. 将节点 B 标记为 `FAIL`（客观下线）状态
				2. 向集群**广播**一条 **FAIL**​ 消息
				3. 所有收到 FAIL 消息的节点都会立即将节点 B 标记为客观下线
				4. 如果 B 是主节点的话，触发后续的故障转移流程
	- 槽位变更
		1. **变更源起**：当管理员执行槽迁移命令（如 `CLUSTER SETSLOT <slot> NODE <target-node-id>`）后，目标节点会首先**增加集群的当前配置纪元（currentEpoch）**，并将其作为自己的新配置纪元（configEpoch）。这个新纪元值通常是集群中最大的。
		2. **信息携带**：节点不再主要依赖消息体，而是直接在 **PING**​ 消息的**消息头**中广播自己负责的槽位位图（`myslots`）和新的、更大的 `configEpoch`。
		3. **协商解决**：其他节点收到 PING 消息后，会对比消息头中的 `configEpoch`和自己本地记录的该槽位所有者的 `configEpoch`。
			- **新纪元更大**：如果发送方的 `configEpoch`更大，接收方就会更新本地槽位映射，承认发送方为新的所有者。
			- **新纪元更小**：如果发送方的 `configEpoch`更小，说明它的信息过时了，接收方可能会回复一个 **UPDATE**​ 消息，告知其最新的槽位分配情况。
		- 特点：滞后更新，大规模集群中槽位信息完全同步需要一定时间

 **故障转移机制**
 1. 故障检测：基于Gossip协议，从主观下线到客观下线
 2. 领导者选举：推举新的主节点
	 - 候选人：客观下线的主节点下的从节点
	 - 竞选机制：
		 - 延迟竞选：从节点不会立即发起选举，而是会等待一个短暂的延迟
			 - 延迟的事件：与从节点的复制偏移量有关
				 - 数据越新（复制偏移量越大）的从节点等待时间越短，从而优先发起选举
			 - 设计目的：拥有最完整数据的节点最有可能成为新主
		 - 投票表决：
			 - 发生时机：作为从节点的自己延迟结束
			 - 采取行动：从节点会向集群中所有**持有哈希槽的主节点**发送投票请求
			 - 投票限制：每个主节点在同一配置纪元内只能投一票
			 - 竞选成功标准：获得**超过半数**选票
 3. 服务恢复：新主节点上任
	 1. 槽位接管
		 - 接管方式：执行`SLAVEOF NO ONE`命令
		 - 命令效果：
			 - 停止复制操作
			 - 将故障主节点负责的所有哈希槽指派给自己
	 2. 广播通知
		 - 通知方式：广播**PONG**消息，消息中携带了新的、更大的配置纪
		 - 产出结果：其他节点收到后，会更新本地的槽位映射表，承认新主节点的地位
	 3. 客户端重定向
		 - 触发时机：客户端尝试访问已故障的旧主节点
		 - 触发反应：收到一个`MOVED`重定向指令
			 - 作用：引导其连接至新的主节点，从而恢复对数据的读写操作

**智能客户端**
- 需求背景：如果客户端每次请求都可能被重定向，就会带来大量的网络往返，增加延迟
- 解决措施：智能客户端
	- 核心思路：为了追求接近直连的性能，客户端不应该每次请求都依赖服务端的重定向
	- 实现基础：在**本地缓存一份“哈希槽-节点”的映射表**
	- 实现方式 在发出请求前就能直接计算出键所在的槽，并找到正确的节点地址
	  
**重定向机制**
- 需求背景：
	- 数据被分散存储在多个节点上，当一个客户端随意连接到集群中的某个节点并发送请求时，这个请求的键所对应的数据，**有很大概率并不存储在当前连接的节点上**
	- 如果每次发生这种情况都需要人工干预或客户端频繁地盲目尝试，会降低集群的可用性和性能
- 解决措施：集群提供一种内置的、自动化的机制，来**引导客户端找到正确的数据所在位置**
- 节点引导客户端重定向的两种方式：
	- MOVED 重定向：永久性迁移
		- 触发场景：当客户端请求了一个错误的节点，并且该请求的键所在的哈希槽已经**永久地**被分配给了另一个节点时
		- 客户端处理：
			1. 智能客户端在收到 `MOVED <slot> <targetIP>:<targetPort>`错误后，会**更新本地的槽位映射缓存**，然后向新节点重新发送命令
			2. 后续所有对该槽的请求都将直接发往新节点。
	- ASK 重定向：临时性迁移
		- **触发场景**：在**数据迁移过程中**，客户端请求的键可能已从源节点迁移到了目标节点。源节点发现自己不再持有该键，但槽位本身仍由它负责时，会返回 ASK 错误
		- **客户端处理**：客户端收到 `ASK <slot> <targetIP>:<targetPort>`错误后，会：
			1. 向目标节点发送一个 `ASKING`命令（这是一个临时许可，告知目标节点处理此请求）。
			2. 再次向目标节点发送原始命令。
			3. **不会更新本地的槽位映射缓存**，因为迁移还没完成




----
## 搭建Redis分片集群
好的，在您现有的一主两从哨兵架构基础上，扩展搭建一个三主三从的Redis分片集群，是一个从“高可用”向“高可用+水平扩展”演进的重要步骤。由于架构有根本性不同，我们需要重新规划。

为了帮助您清晰了解整个搭建流程，我为您梳理了以下主要步骤的路线图：

```
flowchart TD
    A[前期准备与规划] --> B[配置所有Redis节点]
    B --> C[启动节点并创建集群]
    C --> D[验证集群状态与测试]
    D --> E[数据迁移与客户端切换]
```

下面，我们按照这个流程来详细操作。

###  第一步：规划与准备

分片集群的核心是**数据分片**和**高可用**。您现有的三个节点（6379, 6380, 6381）可以作为三个主节点（Master），但一个健壮的“三主三从”集群还需要三个从节点（Slave）。

|角色|IP地址|端口|说明|
|---|---|---|---|
|**Master 1**​|192.168.0.100|6379|利用现有节点|
|**Master 2**​|192.168.0.100|6380|利用现有节点|
|**Master 3**​|192.168.0.100|6381|利用现有节点|
|**Slave 1**​|192.168.0.100|6382|**需要新建**，作为Master 1的从节点|
|**Slave 2**​|192.168.0.100|6383|**需要新建**，作为Master 2的从节点|
|**Slave 3**​|192.168.0.100|6384|**需要新建**，作为Master 3的从节点|

**重要前提：**

- **停止哨兵服务**：为避免干扰，请先停止之前搭建的Redis哨兵服务。
    
- **备份数据**：**此操作会清空现有数据**。请务必先对现有Redis数据进行备份，集群创建完成后再考虑导入。
    

###  第二步：配置Redis节点

我们需要为六个节点创建各自的配置文件。关键配置项如下：

1. **创建配置目录和文件**：
    
    ```
    # 为每个节点创建独立的目录
    for port in 6379 6380 6381 6382 6383 6384; do
        sudo mkdir -p /etc/redis/cluster/${port}
        sudo mkdir -p /var/lib/redis/${port}
    done
    ```
    
2. **创建通用配置文件模板**（例如 `/etc/redis/cluster/redis-cluster.tmpl`）：
    
```
port ${PORT}  
bind 0.0.0.0  
  
# 集群配置  
cluster-enabled yes  
cluster-config-file nodes.conf  
cluster-node-timeout 5000  
  
# 持久化（建议开启）  
appendonly yes  
dir /var/lib/redis/${PORT}/  
  
# 安全设置（请修改为强密码）  
#requirepass your_secure_password_here  
#masterauth your_ecure_password_here  
  
# 关闭保护模式以便于测试，生产环境应配置防火墙  
protected-mode no  
daemonize yes
```
    
**注意**：`requirepass`和 `masterauth`的密码必须相同，这是集群节点间通信的基础。
    
3. **为每个端口生成具体配置**：
    
    ```
    for port in 6379 6380 6381 6382 6383 6384; do
        sudo sed "s/\${PORT}/$port/g" /etc/redis/cluster/redis-cluster.tmpl | sudo tee /etc/redis/cluster/${port}/redis.conf > /dev/null
        sudo chown -R redis:redis /etc/redis/cluster/${port}
        sudo chown -R redis:redis /var/lib/redis/${port}/
    done
    ```
    

###  第三步：启动服务并创建集群

1. **启动所有Redis实例**：
    先开启root
```
    for port in 6379 6380 6381 6382 6383 6384;do
redis-server /etc/redis/cluster/$port/redis.conf
done


```

检查启动情况
```
    ps aux | grep redis-server
```
2. **创建分片集群**：
    
    使用 `redis-cli`命令一键创建集群，`--cluster-replicas 1`参数表示每个主节点分配1个从节点。
    
    ```
    redis-cli --cluster create --cluster-replicas 1 192.168.0.100:6379 192.168.0.100:6380 192.168.0.100:6381 192.168.0.100:6382 192.168.0.100:6383 192.168.0.100:6384 
    ```
    
    执行后，命令行会显示它提议的**主从分配方案**（例如，6382可能是6379的从节点）。请仔细核对，输入 `yes`确认即可完成集群创建。
    - 之前创建失败了，使用ps aux | grep redis-server检查，发现有两个实例的状态是TL(终止)，重新启动后恢复正常

###  第四步：验证与测试

1. **检查集群状态**：
    
    ```
    redis-cli -c -h 192.168.0.100 -p 6379 -a your_secure_password_here cluster nodes
    ```
    
    观察输出，应有3个节点的 `role`是 `master`，3个是 `slave`。并且所有 `16384`个哈希槽（hash slots）都应被分配完毕（`[OK] All 16384 slots covered`）。
    
2. **测试数据分片与高可用**：
    
    - **分片测试**：连接集群（使用 `-c`参数启用集群模式），设置几个不同的key，观察客户端是否自动在节点间重定向。
        
        ```
        redis-cli -c -h 192.168.0.100 -p 6379 -a your_secure_password_here
        127.0.0.1:6379> set key1 value1
        127.0.0.1:6379> set key2 value2 # 这两个key可能会被重定向到不同的节点
        ```
        
    - **高可用测试**：手动停止一个主节点（如6379），等待几秒后再次执行 `cluster nodes`命令。你应该会看到其对应的从节点（如6382）的角色自动从 `slave`提升为 `master`。
        
    
re
###  第五步：数据迁移与客户端切换

这是最关键的一步，关乎业务连续性。

- **数据迁移**：集群创建后是一个空数据集。您需要将之前备份的数据导入到新集群中。
    
    - **如果数据量不大**：可以编写脚本，通过 `redis-cli -c`连接集群，重新写入数据。
        
    - **如果数据量大**：可以考虑使用 `redis-cli --cluster import`命令，或者使用如 `redis-migrate-tool`等专业工具。
        
    
- **客户端配置**：
    
    - **不再需要**配置哨兵节点的地址。
        
    - 应用程序的Redis客户端需要**支持集群协议**。
        
    - 配置时，只需要填写集群中**任意一个或多个节点**的地址（IP:Port）即可，客户端会自动发现整个集群拓扑。例如，在Spring Boot中：
        
    
```
data:  
  redis:  
    cluster:  
      max-redirects: 6  
      nodes:  
        - 192.168.0.100:6379  
        - 192.168.0.100:6380  
        - 192.168.0.100:6381  
        - 192.168.0.100:6382  
        - 192.168.0.100:6383  
        - 192.168.0.100:6384  
    lettuce:  
      pool:  
        max-active: 10  
        max-idle: 10  
        min-idle: 1  
        time-between-eviction-runs: 10s  
jackson:  
  default-property-inclusion: non_null # JSON处理时忽略非空字段
        
        
```

- Redisson配置
```
@Bean  
public RedissonClient redissonClient(){  
    Config config = new Config();  
    config.useClusterServers()  
            .addNodeAddress(  
                    "redis://192.168.0.100:6379",  
                    "redis://192.168.0.100:6380",  
                    "redis://192.168.0.100:6381",  
                    "redis://192.168.0.100:6382",  
                    "redis://192.168.0.100:6383",  
                    "redis://192.168.0.100:6384"  
            )  
            .setScanInterval(2000) // 拓扑刷新间隔  
            .setRetryAttempts(3)   // 命令重试次数  
            .setRetryInterval(1000) // 重试间隔  
            .setTimeout(3000)      // 命令超时  
            .setConnectTimeout(5000); // 连接超时  
  
    return Redisson.create(config);  
}
```
- [[拓扑刷新]]的含义
## Redis集群相关命令
### 添加节点
 将新节点（192.168.0.100:6382）加入现有集群（以192.168.0.100:6379为例）
```
redis-cli -a your_secure_password_here --cluster add-node 192.168.0.100:6382 192.168.0.100:6379
```
![[Pasted image 20251124183430.png]]
- 指定一个分片集群的任一节点，即可定位到其相关联的集群
- 添加的节点默认身份是主节点，但是是一个空节点，没有被分配任何的哈希槽
- 可以使用`--cluster-slave`指定新增加的节点为从节点，并且使用`--cluster-master-id`指定其主节点

### 重新分哈希槽（迁徙哈希槽）
```
# 连接集群中任意节点执行即可
redis-cli -a your_secure_password_here --cluster reshard 192.168.0.100:6379
```
![[Pasted image 20251124192055.png]]
执行命令后，你会进入一个交互式界面，需要依次输入以下信息：

1. **迁移的哈希槽数量**：输入你希望分配给新节点的槽数量。Redis 集群总共有 16384 个槽。例如，在四主节点集群中，若想均分，可分配 4096 个槽 `(16384 / 4)`。
    ![[Pasted image 20251124192111.png]]
2. **目标节点 ID**：输入**新主节点的 ID**。这个 ID 可以在执行 `redis-cli -c -p 6379 cluster nodes`命令查看集群节点信息时找到。
3. **数据来源**：输入从哪里获取这些哈希槽：
    - **输入 `all`**：系统会**自动从所有现有的主节点中平均抽取**指定数量的槽。这是最推荐的方式，能保持集群数据均衡
    - **输入源节点 ID**：你也可以手动输入一个或多个现有主节点的 ID，然后输入 `done`结束指定。这种方式用于精确控制槽的来源。
4. **确认迁移计划**：命令会列出迁移方案，输入 `yes`确认后，迁移过程正式开始。

### 删除节点

####  第一步：确认节点信息

在开始删除之前，必须先明确要删除的节点的角色（主节点还是从节点）和ID。

1.  连接至集群任意节点，执行以下命令查看集群状态：
    ```bash
    redis-cli -c -h 192.168.0.100 -p 6379 cluster nodes
    ```
2.  分析命令输出。输出内容大致如下：
    ```
    a3c45f... 192.168.0.100:6379@16379 master - 0 162... 0 connected 0-5460
    24421f... 192.168.0.100:6380@16380 master - 0 162... 1 connected 5461-10922
    ...
    e9aac3... 192.168.0.100:6382@16382 slave a3c45f... 0 162... 0 connected
    ```
    ◦   角色：`master` 表示主节点，`slave` 表示从节点。

    ◦   节点ID：每行开头的长字符串（如 `a3c45f...`）。

    ◦   哈希槽：主节点行末尾会显示它负责的槽范围（如 `0-5460`）。


请记下你要删除的节点的ID和角色，这至关重要。

#####  第二步：迁移数据（仅针对主节点）

如果你要删除的是一个主节点，必须先将其负责的所有哈希槽迁移到其他主节点上。如果删除的是从节点，可以跳过此步。

假设你要删除端口为 `6380` 的主节点，其节点ID为 `24421f...`，它负责槽 `5461-10922`。

1.  执行重新分片命令：
    ```bash
    redis-cli --cluster reshard 192.168.0.100:6379
    ```
2.  根据提示操作：
    ◦   要移动多少槽？ 输入主节点当前负责的槽总数，例如 `5462`（从5461到10922，共5462个槽）。

    ◦   接收这些槽的节点ID是多少？ 输入集群中另一个主节点的ID（例如 `6379` 的节点ID）。

    ◦   从哪个节点移出这些槽？ 输入你要删除的主节点的ID（即 `6380` 的节点ID：`24421f...`）。

    ◦   输入 `done` 开始迁移。

3.  等待迁移完成。这个过程可能需要一些时间，取决于数据量的大小。

迁移完成后，再次执行 `cluster nodes` 命令，确认该主节点不再负责任何哈希槽（`connected` 后面没有槽范围信息）。

####  第三步：从集群中移除节点

无论删除主节点还是从节点，都使用以下命令。将 `端口号` 和 `节点ID` 替换为你要删除的节点的信息。

```bash
redis-cli --cluster del-node 192.168.0.100:6379 <要删除的节点ID>
```
例如，要删除 `6380` 节点：
```bash
redis-cli --cluster del-node 192.168.0.100:6379 24421f...
```

这个命令会通知集群中的其他节点忘记目标节点，并将其从集群配置中删除。

####  第四步：最终清理

节点从集群中移除后，还需要在操作系统层面进行清理。

1.  停止Redis服务：
    ```bash
    # 连接到要删除的节点并执行关闭
    redis-cli -h 192.168.0.100 -p 6380 shutdown
    ```
    如果上述命令无效，可以使用 `kill` 命令强制终止进程。
2.  （可选）删除数据文件：如果你确定不再需要这个节点的数据，可以删除其工作目录（由配置中的 `dir` 指定）下的所有文件，包括 `dump.rdb`、`appendonly.aof` 和 `nodes.conf`。这一步是不可逆的，请务必谨慎。

####  第五步：验证集群状态

节点删除后，务必检查集群是否健康。

1.  检查节点是否已消失：
    ```bash
    redis-cli -c -h 192.168.0.100 -p 6379 cluster nodes
    ```
    确认输出的节点列表中已经没有了你刚删除的节点。
2.  检查集群状态是否正常：
    ```bash
    redis-cli -c -h 192.168.0.100 -p 6379 cluster info
    ```
    确保输出中 `cluster_state:ok` 并且 `cluster_slots_ok:16384`。

### 数据迁移
![[c499bee10b98fbf442a93b2bf6ba1968.jpeg]]

## Redis集群的问题
- 主从和集群的选择：
	- 单体Redis（主要来自Redis）已经可以达到百万的QPS，并且也具备了非常强的高性能。
	- 如果主从能满足业务需求的情况下，大量不建立Redis集群
### 集群完整性问题
- 需求背景：在Redis的默认配置中，如果发现任意一个插槽不可用，则整个集群都会停止对外服务![[Pasted image 20251126143629.png]]
- 配置：cluster-require-full-coverage 
	- 默认值为：yes
	- 为了保证高可用特性，这里建议将 cluster-require-full-coverage配置为no
### 集群的带宽问题

- 需求背景：集群节点之间会不断的互相Ping来确定集群中其它节点的状态。
	- 每次Ping携带的信息至少包括:
		- 插槽信息
		- 集群状态信息
	- 因此：集群中节点越多，集群状态信息数据量也越大。节点数量过多，每次集群互通需要的带宽会非常高
		- 10个节点的相关信息可能达到1kb
- 解决途径：
	- 减小单个物理机的ping带宽占用：
		- 避免大集群，集群节点数不要太多，最好少于1000。
			-  1000个节点一次ping要100kb的数据
		- 如果业务庞大，则建立多个集群
			- 根据业务拆分集群
		- 避免在单个物理机中运行太多Redis实例，大概十个左右
			- 单个机器Redis实例越多，ping的数据量也是翻倍
	- 减少ping的频率：
		- 配置合适的cluster-node-timeout值
			- cluster-node-timeout 越大，ping的频率会降低
			- cluster-node-timeout 太大，故障发现的能力变弱，可用性降低。
### [[集群的数据倾斜问题]]
- 数据倾斜的表现：
	- 存储倾斜：某些节点内存使用率远高于其他节点
	- 流量倾斜：某些节点QPS/带宽使用率异常高
	- CPU倾斜：某些节点CPU使用率持续高位
