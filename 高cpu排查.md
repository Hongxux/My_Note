1. ​**监控告警**​：监控系统告警 CPU 95%+。
    
2. ​**定位进程**​：`top`命令找到高 CPU 进程 PID。
    
3. ​**定位线程**​：`top -Hp PID`找到高 CPU 线程 TID，并转换为十六进制。
    
4. ​**分析堆栈**​：`jstack PID | grep -A 20 <十六进制TID>`查看该线程在做什么。
    
5. ​**根因分析**​：
    
    - ​**GC 问题**​ -> 优化 JVM 参数，排查内存泄漏。
        
    - ​**代码热点**​ -> 优化算法，异步化，增加缓存。
        
    - ​**锁竞争**​ -> 优化锁的使用。
        
    
6. ​**解决与预防**​：立即解决后，复盘原因，完善监控和代码设计，避免复发。
### 第一阶段：诊断与定位 - 找到“元凶”

#### 步骤 1：定位高 CPU 进程

首先，我们需要确定是哪个进程导致了 CPU 过高。

​**命令**​：`top`

​**操作**​：

1. 在服务器上直接运行 `top`命令。
    
2. 看第一行负载情况，再看 `%CPU`这一列，按 `P`（大写） 键可以按 CPU 使用率降序排列。
    
3. ​**目标**​：找到 CPU 使用率最高的进程，记下它的 ​**PID（进程ID）​**。
    

​**结果解读**​：

- 如果是你的 Java 应用（如 `java -jar ...`），那么进入 ​**步骤 2**。
    
- 如果是 `mysql`, `redis`或其他进程，则重点排查这些服务。
    
- 如果是未知进程，可能是挖矿病毒等，需要立即排查。
    

#### 步骤 2：定位高 CPU 线程（关键步骤）

一个进程下有多个线程，我们需要找到是哪个线程在“疯狂”占用 CPU。

​**命令**​：`top -Hp <PID>`或 `ps -eLf | grep <PID>`

​**操作**​：

1. 将 `<PID>`替换为你在上一步找到的进程 ID。
    
2. 在 `top -Hp`的视图中，同样按 `P`键排序，找到消耗 CPU 最高的线程。
    
3. ​**目标**​：记下这个高 CPU 线程的 ​**TID（线程ID）​**。​**注意**​：需要将这个 TID 转换为**十六进制**，因为后续的线程堆栈信息中的线程 ID 是十六进制的。
    
    - 转换命令：`printf "%x\n" <TID>`
        
    

#### 步骤 3：分析线程堆栈，找到问题代码

现在，我们知道了是哪个线程，但要搞清楚这个线程在**执行什么代码**。

​**命令（针对 Java 进程）​**​：`jstack`

​**操作**​：

1. `jstack <PID> > jstack.log`# 导出当前进程的所有线程堆栈信息到一个文件。
    
2. 用文本编辑器打开 `jstack.log`，用你在上一步得到的**十六进制 TID**​ 去文件里搜索。
    
3. ​**目标**​：找到对应的线程，查看它的**调用堆栈**，即它正在执行哪个类的哪个方法。
    

---

### 第二阶段：根因分析与解决方案 - 解决“为什么”

根据线程堆栈信息，我们通常会遇到以下几种经典场景：

#### 场景一：JVM 频繁垃圾回收（GC）

- ​**线程堆栈特征**​：堆栈中可能包含 `GC`相关的线程，如 `GC task thread`。但更可靠的判断方法是使用 JVM 内置工具。
    
- ​**确认命令**​：`jstat -gcutil <PID> 1000 5`（每秒钟打印一次GC情况，共5次）
    
    - 关注 `FGC`（Full GC 次数）和 `FGCT`（Full GC 时间）。如果 `FGC`在短时间内急剧增加，且 `FGCT`很高，说明正在频繁进行Full GC，而Full GC会停止所有应用线程（Stop-The-World），导致CPU疯狂工作。
        
    
- ​**解决方案**​：
    
    1. ​**应急**​：立即通过监控平台或 `jstat`确认 GC 问题。
        
    2. ​**根治**​：
        
        - ​**调整 JVM 参数**​：优化堆内存大小（`-Xms`, `-Xmx`），调整新生代和老年代的比例（`-XX:NewRatio`），选择合适的GC算法（如G1）。
            
        - ​**排查内存泄漏**​：使用 `jmap -histo:live <PID>`或 `jmap -dump:format=b,file=heap.hprof <PID>`导出堆内存快照，用 MAT 等工具分析是什么对象占用了大量内存且无法被回收。
            
        
    

#### 场景二：业务代码存在“热点”或无限循环

- ​**线程堆栈特征**​：线程堆栈会**卡在某个业务方法**上。例如，堆栈显示线程一直处于 `RUNNABLE`状态，并且一直在执行某个计算密集型的方法（如复杂的数学运算、正则表达式、循环处理大量数据）。
    
- ​**解决方案**​：
    
    1. ​**应急**​：如果可能，立即**重启服务**以快速恢复，同时保留现场（如 `jstack`日志和 `heap dump`）。
        
    2. ​**根治**​：
        
        - ​**代码优化**​：优化算法逻辑，减少循环嵌套或次数。
            
        - ​**异步处理**​：将耗时的计算任务放入线程池异步执行，避免阻塞主线程。
            
        - ​**增加缓存**​：对计算结果进行缓存，避免重复计算。
            
        - ​**限流降级**​：在网关或入口层面对接口进行限流，防止异常请求打满CPU。
            
        
    

#### 场景三：死锁或锁竞争激烈

- ​**线程堆栈特征**​：使用 `jstack`命令时，它会在最后**自动检测死锁**。如果发现死锁，会明确提示 `Found one Java-level deadlock:`。对于锁竞争，可能会看到大量线程阻塞在同一个锁上（状态为 `BLOCKED`）。
    
- ​**解决方案**​：
    
    1. ​**分析堆栈**​：根据 `jstack`输出的死锁信息，找到互相等待锁的线程和对应的代码。
        
    2. ​**修复代码**​：调整锁的获取顺序，使用并发工具类（如 `ConcurrentHashMap`）替代简单的同步锁，或减小锁的粒度。
        
    

---

### 第三阶段：高级工具与预防措施

#### 1. 使用 Profiler 工具进行深度 profiling

- ​**Arthas**​：阿里开源的 Java 诊断神器，非常适合在线诊断。
    
    - `thread <十六进制TID>`：直接查看指定线程的堆栈。
        
    - `profiler start`/ `profiler stop`：生成 CPU 火焰图，直观展示 CPU 时间都花在了哪些方法上。
        
    
- ​**Async-Profiler**​：生成 CPU 或内存火焰图的强大工具，对性能影响极小。
    

#### 2. 建立监控与告警体系（治本之策）

- ​**应用监控**​：集成 APM 工具（如 SkyWalking, Pinpoint），实时监控应用的 CPU、内存、GC 、QPS、慢请求等指标。
    
- ​**系统监控**​：使用 Prometheus + Grafana 监控服务器的基础资源（CPU、内存、磁盘 IO、网络 IO）。
    
- ​**设置告警**​：当 CPU 使用率持续超过 80% 或 GC 频率异常时，立即通过短信、钉钉、微信等渠道通知负责人。
    

