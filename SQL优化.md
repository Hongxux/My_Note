---
aliases:
  - SQL语句优化
  - 优化SQL
---
### 插入数据

**500-1000条批量插入**

#### 1. 批量插入

- ​**做法**​：一条 `INSERT`语句插入多条数据（多个values），而非多条单行插入语句。
    
- ​**优化原理**​：如流程图所示，这种方法最直接的收益是**大幅减少了应用与数据库服务器之间的网络往返次数**。对于需要插入大量数据的情况，这能极大降低网络延迟开销。此外，数据库服务器对一条SQL语句的解析和执行开销是一次性的，因此批量处理比逐条执行效率高得多。
    

#### 2. 手动提交事务

- ​**做法**​：在插入操作前使用 `START TRANSACTION`（或 `BEGIN`），在所有插入完成后执行 `COMMIT`一次性提交。
    
- ​**优化原理**​：在默认的自动提交模式下，每条 `INSERT`都会被视为一个独立的事务，这意味着数据库需要保证每条语句都持久化到磁盘（写事务日志）。如流程图所示，手动开启一个事务，​**将所有插入操作放在一个事务中，最后统一提交，可以将多次昂贵的磁盘日志刷盘操作减少到一次**，从而极大提升了效率。
    

#### 3. 主键顺序插入

- ​**做法**​：插入数据时，主键ID的值尽量保持递增顺序（如1, 2, 3...），而不是无序（如8, 1, 9...）。
    
- ​**优化原理**​：绝大多数数据库的表数据是基于主键组织的B+Tree索引。如流程图所示，​**顺序插入主键时，新数据总是追加到索引树的末尾一页，不会涉及中间页的分裂和移动**。而乱序插入可能导致数据库需要频繁地为已满的索引页进行分裂、重新平衡，这会带来大量的随机I/O和性能开销。
    

#### 总结与实践建议

在实际开发中，尤其是需要批量导入大量数据时，​**建议将这三种策略结合使用**，以达到最优的插入性能：

1. ​**开启一个事务**。
    
2. 在事务内，使用 ​**批量插入**​ 语句。
    
3. 确保批量数据中的**主键ID是连续递增的**。
    
4. 最后**提交事务**。
    

通过这种方式，可以最大限度地减少网络交互、事务日志I/O以及索引维护开销，使数据插入操作达到最佳性能。


#### 使用 LOAD DATA 进行大批量数据插入

当需要插入数十万甚至上百万条数据时，使用多条 `INSERT`语句性能会非常低下。此时，应该使用 MySQL 专为这种场景提供的 `LOAD DATA`命令。(优化多个数量级)

`LOAD DATA INFILE`是 MySQL 数据迁移和批量导入中最快速的方法。其核心步骤可以概括为：​**开启功能 -> 指定文件 -> 匹配格式**。掌握这一工具，对于处理海量数据导入任务至关重要。
![[Pasted image 20251030215810.png]]
![[Pasted image 20251030215816.png]]
##### 为什么 LOAD DATA 更快？

- ​**减少开销**​：它将所有数据打包在一次操作中，极大减少了 SQL 解析、事务处理、日志写入等开销。
    
- ​**直接加载**​：相比执行数万条 SQL 语句，它更接近于直接向数据文件加载数据。
    

##### 操作步骤详解

图中清晰地列出了三个必要步骤：

1. ​**客户端连接时启用本地文件加载能力**​
    
    在通过命令行连接 MySQL 服务器时，必须加上 `--local-infile`参数。
    
    ```
    mysql --local-infile -u root -p
    ```
    
2. ​**服务器端开启全局开关**​
    
    成功连接后，需要在 MySQL 中设置一个全局变量，开启从本地加载文件的功能。
    
    ```
    SET GLOBAL local_infile = 1;
    ```
    
3. ​**执行 LOAD DATA 指令**​
    
    这是最核心的一步，指令格式如下：
    
    ```
    LOAD DATA LOCAL INFILE '本地数据文件路径'
    INTO TABLE 目标表名
    FIELDS TERMINATED BY '字段分隔符'  -- 图中为逗号 ','
    LINES TERMINATED BY '行分隔符';   -- 图中为换行符 '\n'
    ```
    
    - ​**`LOCAL INFILE '/root/sql.log'`**​：指定数据文件在客户端机器上的路径。
        
    - ​**`INTO TABLE 'tb_user'`**​：指定数据要插入的目标表。
        
    - ​**`FIELDS TERMINATED BY ','`**​：指明数据文件中每个字段是用什么符号分隔的（图中是逗号分隔的 CSV 格式）。
        
    - ​**`LINES TERMINATED BY '\n'`**​：指明每一行记录是用什么符号分隔的（图中是换行符 `\n`）。
        
    

##### 数据文件格式要求

图片左侧的表格数据展示了数据文件（如 `sql.log`）应有的格式，这是一个标准的 CSV 格式：

```
1,jdtmmKQLwu1,jdtmmKQLwu,jdtmmKQLwu,2020-10-13,1
2,BTJOeWjRiw2,BTJOeWjRiw,BTJOeWjRiw,2020-6-12,2
...
```

每一行是一条记录，字段之间用逗号分隔，记录之间用换行符分隔。数据的顺序必须与数据库表结构的列顺序一致。




---
### 主键优化

##### InnoDB 逻辑存储结构
![[Pasted image 20251030221249.png]]
- **表空间**​： 最大的逻辑单位，相当于一个**数据库文件**​（如 `ibdata1`文件或独立的 `.ibd`文件）。
    
- ​**段**​： 表空间的主要组成部分，最重要的有**数据段**​（Leaf node segment，存放实际行数据）和**索引段**​（Non-leaf node segment，存放索引数据）。一个索引会生成两个段。
    
- ​**区**​： 由连续的**页**组成，大小为 ​**1MB**。这是空间分配的基本单位，为了减少随机I/O。
    
- ​**页**​： InnoDB 磁盘管理的最小单位，也是内存与磁盘交互的基本单位，大小固定为 ​**16KB**。一个区包含 ​**64个页**​（1MB / 16KB = 64）。
    
- ​**行**​： 页内部存储的每一行用户数据。
#### 页分裂现象（乱序插入的后果）
- **页分裂**​： 这是与“页合并”相对的过程。当向一个**已满**的数据页插入新数据时，InnoDB 需要申请一个新的数据页，并将原页的部分数据移动到新页，以容纳新插入的数据。这个过程称为**页分裂**。
    
- ​**为何要避免页分裂**​： 页分裂是一个**昂贵的操作**，会导致：
    
    1. ​**性能下降**​： 涉及磁盘分配和数据移动。
        
    2. ​**空间浪费**​： 页分裂后，每个页的填充率会降低（通常约为50%）。
        
    3. ​**数据碎片化**​： 降低连续读取的效率。
 
顺序插入
![[Pasted image 20251030220521.png]]

乱序插入
![[Pasted image 20251030220531.png]]

![[Pasted image 20251030220642.png]]

##### 设计原则
1. ​**降低主键长度**​： 主键值会出现在所有非主键索引的叶子节点中。主键长度越短，普通索引占用的空间就越小，整体性能越高。
    
2. ​**顺序插入（AUTO_INCREMENT）​**​： ​**这是最关键的原则**。使用自增主键，新数据总是追加到 B+Tree 的末尾，最大程度避免了中间数据页的分裂。
    
3. ​**避免使用无序主键（如 UUID）​**​： UUID 是随机的，新插入的数据可能位于 B+Tree 的中间位置，极易引发页分裂，严重破坏性能。
    
4. ​**避免修改主键**​： 修改主键代价高昂，可能涉及B+Tree中节点位置的调整，甚至引发页分裂。
#### 页合并
- ​**逻辑删除**​： 当执行 `DELETE`时，数据并非被立刻物理擦除，而是被**标记为删除**，其空间允许被新插入的数据复用。
    
![[Pasted image 20251030220841.png]]
![[Pasted image 20251030220943.png]]

- ​**触发条件**​： 当一页中被标记删除的记录空间达到一个阈值（**MERGE_THRESHOLD**，默认为页大小的 50%，即约8KB）（可以在创建表的时候设置）时，InnoDB 会尝试进行**页合并**。
    
- ​**合并过程**​： InnoDB 会寻找相邻的页（前一个或后一个），看看能否将当前页的有效数据合并到相邻页中，从而使当前页变为空页，可以被充分复用。
    

​**核心价值**​： ​**页合并是 InnoDB 重要的自我优化机制**，它能有效减少空间浪费，保持数据页的紧凑性，从而提升查询效率


----
### order by 优化（转化成Using Index）
如何通过**创建合适的复合索引**来优化 `ORDER BY`排序查询的性能，避免低效的 `FileSort`操作，转而使用高效的 `Using index`方式

#### （1）`Using filesort`（低效排序）

- ​**触发条件**​：当查询的排序条件无法直接通过索引满足时（例如，没有索引或索引顺序不匹配）。
    
- ​**工作原理**​：MySQL 需要执行以下步骤：
    
    1. 扫描表数据（全表扫描或索引扫描）找到满足条件的行。
        
    2. 将这些行加载到内存的**排序缓冲区（sort buffer）​**中。
        
    3. 在内存中对数据进行排序（如果数据量太大，可能涉及磁盘临时文件）。
        
    
- ​**性能问题**​：这是一个 CPU 和内存密集型操作，尤其对于大数据集性能很差。
    

#### （2）`Using index`（高效排序）

- ​**触发条件**​：当查询的排序条件与索引的顺序完全匹配时（例如，索引是 `(age, phone)`，查询按 `age, phone`排序）。
    
- ​**工作原理**​：MySQL 可以直接按索引的顺序扫描数据，无需额外排序。
    
- ​**优势**​：数据返回时已经有序，效率极高。


#### （3）`Using filesort`优化成`Using index`

1. 要创建索引，且要指定索引中字段的排序顺序
```sql
	CREATE INDEX idx_user_age_pho_ad ON tb_user(age ASC, phone DESC);
```
2. order by要符合最左前缀原则，而且排序方式要与索引的创建保持一致
3. 使用覆盖索引，避免回表
4. 如果不可避免使用`Using filesort`，则可以增加排序缓冲区大小sort_buffer_size
5. 使用explain语句在extra中能知道使用的是哪种排序方式

索引中Collation字段代表排序方式，A代表升序，D代表倒序

![[Pasted image 20251031141612.png]]

---
### group by 优化

​**为 `GROUP BY`的字段创建符合最左前缀法则的复合索引，可以避免耗时的文件排序和临时表操作，从而极大提升查询效率。​**​
在设计数据库时，如果预见到某些字段会频繁用于 `GROUP BY`操作，应优先为其创建合适的索引。这张图提供了一个非常实用的优化范例。
1. ​**无索引状态（性能低下）​**​
    
    ```
    -- 首先删除可能存在的索引
    DROP INDEX idx_user_pro_age_sta ON tb_user;
    -- 然后执行分组查询并查看执行计划
    EXPLAIN SELECT profession, COUNT(*) FROM tb_user GROUP BY profession;
    ```
    
    ​**预期结果与问题**​：此时，由于 `profession`字段上没有索引，数据库为了完成分组，必须进行全表扫描，并在内存或磁盘上创建一个**临时表**来存放中间结果，再进行排序和分组。这会在 `EXPLAIN`结果的 `Extra`列显示 ​**`Using temporary; Using filesort`**，这是性能瓶颈的标志。
    
2. ​**创建复合索引后（性能优化）​**​
    
    ```
    -- 创建一个以分组字段为起始的复合索引
    CREATE INDEX idx_user_pro_age_sta ON tb_user(profession, age, status);
    -- 再次执行相同的分组查询
    EXPLAIN SELECT profession, COUNT(*) FROM tb_user GROUP BY profession;
    ```
    
    ​**优化原理与结果**​：创建了以 `profession`为最左列的索引后，数据库可以直接**按索引的顺序进行扫描**。因为索引已经将相同 `profession`的数据物理上连续存放在一起了，数据库只需顺序扫描索引，即可高效完成分组计数。此时 `EXPLAIN`的 `Extra`列会显示 ​**`Using index`**，表示实现了最高效的“松散索引扫描”。
    
3. ​**验证最左前缀法则**​
    最左前缀法则：可以在where中有age（最左），也可以在order中有age
    ```
    -- 尝试按复合索引的前两列进行分组
    EXPLAIN SELECT profession, COUNT(*) FROM tb_user GROUP BY profession, age;
    ```
    
    ​**原理与结果**​：这个分组条件 `(profession, age)`完全符合索引 `(profession, age, status)`的**最左前缀法则**。索引数据先按 `profession`排序，再按 `age`排序，这正好满足分组需求。因此，这个查询同样可以高效地使用索引。
    
    

#### 关键要点总结

1. ​**索引是优化 GROUP BY 的根本**​：为分组字段建立索引，特别是将其作为复合索引的最左列，是提升性能最有效的方法。
    
2. ​**最左前缀法则至关重要**​：`GROUP BY`的条件必须与索引定义的顺序一致，且从最左列开始，不能跳过中间列。
    
3. ​**目标**​：优化的目标是让 `GROUP BY`查询的执行计划从出现 ​**`Using temporary; Using filesort`**​ 转变为 ​**`Using index`**。
    

---
### limit 优化（覆盖查询+子查询）

大数据量进行分页时候，页数越后面，耗时越长

优化：
```sql
-- 1. 子查询：利用覆盖索引，只查主键ID，快速定位起始位置
SELECT id 
FROM tb_user 
ORDER BY id 
LIMIT 1000000, 10; -- 这一步在索引中很快，因为索引树很小

-- 2. 主查询：通过精确的主键ID，快速拿到完整数据
SELECT id, name, age, status 
FROM tb_user u,(SELECT id 
    FROM tb_user 
    ORDER BY id 
    LIMIT 1000000, 10) a
WHERE u.id = a.id;
;
```

#### 一、 什么时候考虑 `LIMIT`优化？

当你的分页查询出现以下情况时，就必须考虑优化：

1. ​**深度分页时性能急剧下降**​：这是最显著的信号。查询 `LIMIT 10000, 20`比 `LIMIT 0, 20`慢几个数量级。
    
2. ​**数据量巨大**​：当表数据达到百万、千万级别时，即使翻前几页，简单的 `LIMIT`也可能成为瓶颈。
    
3. ​**高并发场景**​：在用户访问量大的系统中，一个低效的分页查询可能拖垮整个数据库。
    

​**核心原则**​：`LIMIT`本身不是慢查询的元凶，​**`OFFSET`（偏移量）才是**。

#### 二、 为什么页数越往后越慢？
#未理解

​**根本原因**​：`OFFSET 1000000`意味着数据库必须**先读取、排序并跳过前100万条记录**，然后才能拿到你想要的那10条。这个“跳过”的过程需要扫描大量的索引和数据，产生巨大的CPU和I/O开销，即使最后只返回很少的数据。



#### 三、 覆盖索引 + 子查询优化法（最优方案）

使用覆盖索引：​**利用索引的有序性避免排序**
子查询：**将大数据量的扫描转化为小数据量的精确查询**

这种方法的核心思想是：​**不要让数据库去扫描和跳过巨大的 `OFFSET`，而是通过索引直接定位到数据的起始点。​**​

##### 优化前（慢查询）

```
-- 传统的深度分页，非常慢！
SELECT id, name, age, status 
FROM tb_user 
ORDER BY id 
LIMIT 1000000, 10; -- 需要先扫描100万条记录
```

##### 优化后（高效查询）

```
-- 1. 子查询：利用覆盖索引，只查主键ID，快速定位起始位置
SELECT id 
FROM tb_user 
ORDER BY id 
LIMIT 1000000, 10; -- 这一步在索引中很快，因为索引树很小

-- 2. 主查询：通过精确的主键ID，快速拿到完整数据
SELECT id, name, age, status 
FROM tb_user 
WHERE id IN (
    SELECT id 
    FROM tb_user 
    ORDER BY id 
    LIMIT 1000000, 10
);
```

​**或者使用 JOIN 写法（效果相同）​**​：

```
SELECT t.* 
FROM tb_user t
JOIN (
    SELECT id 
    FROM tb_user 
    ORDER BY id 
    LIMIT 1000000, 10
) AS tmp ON t.id = tmp.id;
```

##### 为什么这个优化如此有效？

1. ​**第一步（子查询）​**​：
    
    - 查询的字段只有 `id`，而 `id`上有主键索引。这形成了一个**覆盖索引**。
        
    - 数据库只需要在**小而快的索引B+Tree**中进行扫描和跳过100万条记录，而不需要访问庞大的数据行。
        
    - 虽然也要跳过100万条，但索引树通常比数据文件小得多，操作在内存中进行，速度极快。
        
    
2. ​**第二步（主查询）​**​：
    
    - 子查询返回的是10个精确的 `id`。
        
    - 主查询通过 `WHERE id IN (...)`或 `JOIN`使用主键索引进行等值查询，这是数据库最快的操作之一。
        

#### 四、 其他优化方案

##### 1. 基于游标的分页（推荐用于“无限加载”）

如果不需要随机跳页（如第5页→第100页），而是连续翻页（如“加载更多”），这是最佳方案。

```
-- 第一页
SELECT id, name, age, status 
FROM tb_user 
ORDER BY id 
LIMIT 20;

-- 假设上一页最后一条记录的id是 10020
-- 下一页
SELECT id, name, age, status 
FROM tb_user 
WHERE id > 10020  -- 直接定位，完全不需OFFSET
ORDER BY id 
LIMIT 20;
```

​**优点**​：速度恒定，与页数无关。

​**缺点**​：无法直接跳转到任意页面。

##### 2. 业务层缓存

缓存前几页的热门数据，因为用户大部分时间在浏览前几页。

### 总结

|方案|原理|适用场景|
|---|---|---|
|​**覆盖索引+子查询**​|利用覆盖索引定位，主键查询取数据|需要随机跳转的深度分页|
|​**游标分页**​|记录上一页末尾，用WHERE条件查询下一页|连续翻页（如无限加载）|
|​**业务层缓存**​|缓存热门页面的结果|前几页访问频繁的场景|

---
### count 优化（传值和非空判断）
由于InnoDB引擎导致的，**性能建议**​：对于需要频繁计数的大表，如果业务允许（如不需要事务支持），可考虑使用 MyISAM；但 InnoDB 是默认引擎，更推荐通过索引优化或缓存策略来改善 `COUNT`性能。
- ​**MyISAM 引擎**​：
    - 将表的总行数直接存储在磁盘上（在元数据中）。
    - 执行 `COUNT(*)`时，直接返回这个预存的值，​**效率极高**，时间复杂度为 O(1)。
    - 但注意：如果查询包含 `WHERE`条件，MyISAM 仍需逐行扫描。

- ​**InnoDB 引擎**​：
    - ​**不会预存总行数**，因为支持事务和并发控制，不同事务可能看到不同的行数（如 MVCC 机制）。
    - 执行 `COUNT(*)`时，需要逐行读取数据（可能使用索引）并累积计数，​**效率较低**，时间复杂度为 O(n)。
    - 优化：如果存在可用的二级索引，InnoDB 会优先扫描较小的二级索引来计数（因为二级索引叶子节点包含更少数据）。
​

count是用来统计**选定的字段是非空**的行数
- 有效行数：**选定的字段是非空**

#### COUNT 函数的几种用法及区别

1. **优先使用 `COUNT(*)`**​：
    - 它是 SQL 标准写法，清晰表达“统计行数”的意图，且现代优化器会高效处理。
    - 避免使用 `COUNT(字段)`除非需排除 NULL 值。
        
2. ​**InnoDB 表的优化方法**​：
    - ​**索引优化**​：为经常用于 `WHERE`条件的字段创建索引，使 `COUNT`能利用索引扫描（如 `WHERE status = 'active'`时，索引 on `status`可加速）。
    - ​**避免全表扫描**​：如果表很大，`COUNT(*)`可能较慢，考虑使用近似值（如从信息模式表 `information_schema.tables`获取估算值）或业务层缓存。

| 用法                | 含义                                     | 是否统计 NULL | 性能说明                                           |
| ----------------- | -------------------------------------- | --------- | ---------------------------------------------- |
| ​**`COUNT(*)`**​  | 统计所有行数（包括 NULL 行）。                     | 是         | InnoDB 中需扫描表，但优化器会优先选择最小的索引。                   |
| ​**`COUNT(主键)`**​ | 统计主键字段非 NULL 的行数（主键必非 NULL，故等价于总行数）。   | 否         | 与 `COUNT(*)`效果相同，但可能略慢，因为需检查主键值。               |
| ​**`COUNT(字段)`**​ | 统计指定字段非 NULL 的行数。                      | 否         | 如果字段有索引，可能使用索引扫描；否则全表扫描。                       |
| ​**`COUNT(1)`**​  | 统计常量表达式非 NULL 的行数（1 永远非 NULL，故等价于总行数）。 | 否         | 性能与 `COUNT(*)`类似，但推荐使用 `COUNT(*)`（更符合 SQL 标准）。 |

**区别在于要不要传值、要不要判断非空**
- `COUNT(字段)`需要**传值 + 判断**，开销最大。
- `COUNT(主键)`需要**传值**，开销次之。
- `COUNT(1)`和 `COUNT(*)`​**不传值**，只计数，开销最小。

---
### update 优化（利用索引避免表锁使用行锁）

`InnoDB的行锁是针对索引加的锁, 不是针对记录加的锁, 并且该索引不能失效, 否则会从行锁升级为表锁。`
​**InnoDB 的行锁依赖于索引的有效性**。在设计数据库和编写 SQL 时：
- 确保写操作的 WHERE 条件使用索引（尤其是主键或复合索引）。
- 监控索引失效场景，避免意外升级为表锁。
- 通过合理的索引设计，可以大幅提升数据库的并发处理能力。
    


---

#### 一、InnoDB 行锁的工作原理

InnoDB 的行级锁机制是基于索引实现的，这意味着：

- ​**锁的粒度**​：当执行 UPDATE、DELETE 等写操作时，InnoDB 会尝试只锁定受影响的数据行（而不是整个表）。
    
- ​**锁的依赖**​：行锁直接加在**索引项**上，而不是物理数据行上。如果查询条件使用了索引，InnoDB 可以通过索引快速定位到需要锁定的行。
    

#### 二、更新的where字段

1. ​**使用主键索引的更新（高效、行锁）​**​：
    
    - `UPDATE ... WHERE id = 1`：`id`是主键，默认有主键索引。
        
    - ​**锁行为**​：InnoDB 会直接在主键索引树上锁定 `id=1`对应的索引项，仅锁定这一行。其他事务可以并发访问表中其他行，不会阻塞。
        
    - ​**性能**​：高效，支持高并发。
        
    
2. ​**使用普通字段的更新（需谨慎）​**​：
    
    - `UPDATE ... WHERE name = '韦一笑'`：如果 `name`字段没有索引，或索引失效：
        
        - ​**索引有效时**​：如果 `name`字段有索引，InnoDB 会锁定 `name='韦一笑'`对应的索引项（行锁）。
            
        - ​**索引失效时**​：如果 `name`字段无索引，或查询导致索引失效（如使用函数、类型转换），InnoDB 无法快速定位行，会**退化为表级锁**​（锁定整个表）。
            
        
    

#### 三、为什么“索引失效会导致行锁升级为表锁”？

- ​**索引失效的常见场景**​：
    
    - 字段无索引。
        
    - 查询条件使用函数（如 `WHERE UPPER(name) = '韦一笑'`）。
        
    - 数据类型不匹配（如字符串字段用数字查询）。
        
    - 模糊查询以通配符开头（如 `WHERE name LIKE '%韦'`）。
        
    
- ​**后果**​：
    
    - 一旦退化为表锁，该表的任何写操作（甚至其他行的更新）都会被阻塞，并发性能急剧下降。
        
    - 例如：如果 `name`无索引，执行 `UPDATE student SET no='...' WHERE name='韦一笑'`会锁定整个 `student`表，其他事务无法同时更新同一张表。
        
    

#### 四、实践建议与优化策略

1. ​**为高频查询条件创建索引**​：
    
    - 如果经常使用 `name`字段进行更新或查询，应为 `name`字段创建索引：
        
        ```
        CREATE INDEX idx_name ON student(name);
        ```
        
    - 这能确保行锁生效，避免表锁。
        
    
2. ​**避免索引失效的操作**​：
    
    - 在 WHERE 子句中避免对索引列使用函数、计算或类型转换。
        
    - 例如，改用直接值匹配：`WHERE name = '韦一笑'`而不是 `WHERE name = 123`（如果 name 是字符串）。
        
    
3. ​**优先使用主键或唯一索引**​：
    
    - 主键索引必然触发行锁，是并发更新的最优选择。
        
    


  
---
