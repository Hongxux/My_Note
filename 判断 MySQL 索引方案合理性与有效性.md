
- 核心目标：**降低查询核心开销（尤其是磁盘IO）**，同时平衡维护成本。
	- 判断的根本准则：
		- 数据库性能瓶颈的核心是「磁盘IO」（随机IO延迟是内存的10万倍），索引方案的首要目标是「**减少IO次数、优先用顺序IO替代随机IO**」；
		- 避免“为建索引而建索引”，若方案无法降低核心查询开销，即便结构规范，也无实际意义。
	- 判断的逻辑：
		1. **第一步：看收益**：通过「**过滤效率+排序效率+回表开销**」带来的查询耗时减少、IO/CPU 开销降低，量化查询性能提升，是否能显著减少IO次数（核心）；
		2. **第二步：看成本**：评估「**写入带来的维护开销+存储成本**」，是否在业务可接受范围内，收益＞成本；
- 基于的原理：[[MySQL查询语句的执行流程]]
-  维度1：查询效率维度（核心收益，量化判断优先）
	1. 过滤效率：索引快速筛选出目标数据、高效排除无关数据的能力，筛选越精准、执行速度越快，过滤效率就越强
		- **索引的区分度**：决定过滤的精确性，高精准筛选能最小化 “无效数据的连带处理开销”
			- **有效条目**：满足「所有查询条件」且「最终需要返回给应用」的索引条目（如分页查询中 `LIMIT 10` 对应的 10 条条目）
			- **无效条目**：
				- 定义：**满足`WHERE`筛选条件**，但因**业务需求**（如分页、排序、多条件精准匹配）**无需最终返回**，却需要额外处理的条目」
				- 存在的问题：这些条目虽符合筛选条件，但会产生 **“扫描、CPU 比较、内存占用” 等连带开销**，属于 “处理成本大于价值” 的条目，并非 “不满足查询条件”。
			- **连带开销**：
				- **回表 IO 开销**：若需 “索引过滤 + 二次回表”：区分度越高 → 索引过滤后得到的主键 ID 数量越少 → 回表次数（IO 开销）越少 → 二次过滤的无效数据越少 → 过滤效率越强；
				- **索引扫描的 CPU / 内存开销**：
					- 减少索引**扫描**的条目数，降低**磁盘 IO** 与**内存遍历**开销：覆盖索引的 B + 树叶子节点按索引值有序排列，查询时需扫描 “**匹配**`WHERE`条件的所有索引条目”，因此需要**将其从磁盘中加载到内存中**。在此过程，区分度越高，**需要加载的数据越少**，则磁盘IO次数越少，且内存占用越少。
						- 高区分度的条目少，这些条目大概率在同一个或少数几个索引页中，甚至可能被缓存，因此磁盘IO少
						- 低区分度的条目多，分在成百上千个索引页中，需要多次磁盘IO才能读取完
						- 低区分度的衍生问题：**缓冲池内存**占用提高，造成**缓存淘汰开销**：
							- 低区分度的要占用大量缓冲池空间，可能导致其他核心查询的索引页被淘汰，后续查询需重新加载（触发额外磁盘 IO）
					- 降低 CPU **比较**开销，提升筛选效率：索引扫描时，CPU 需逐一对索引条目进行 “索引值匹配”（如对比`email='xxx'`），筛选出有效条目。高区分度减少了 CPU 的比较次数，低区分度则需反复比较大量无效条目。
					- 优化**排序 / 分页**效率：
						- 覆盖索引查询 `SELECT name FROM user WHERE phone LIKE '138%' LIMIT 10`（phone 前缀选择性 85%）vs `SELECT name FROM user WHERE gender=1 LIMIT 10`（gender 选择性 50%）：
						- 前者：扫描**前 20 条条目**即可筛选出 10 条有效数据，无需额外处理；
						- 后者：**需扫描 20 万条条目**（gender=1 的所有条目），**再**截取前 10 条，即使无需回表，内存遍历和 CPU 筛选的开销也极大，且可能触发磁盘临时存储（若条目数超过`sort_buffer_size`）。
			- 索引区分度的原理：最小化”这些**无效数据**的**连带处理开销**“
			- 量化标准：
				- 基础门槛：保证索引有 “有效收益”：索引能减少至少 **80%** 的扫描数据量时，收益通常覆盖开销
					- 唯一索引：收益极高，几乎无需考虑选择性（选择性 = 1）
						- 唯一索引（如主键、唯一约束）能实现精准查询，且写入时的唯一性校验开销极小，**只要查询频率不是极低，收益均大于开销。**
					- 联合索引：需关注 “整体选择性”，而非单个字段
						- 联合索引的量化标准以 “整体选择性” 为准（如`性别+年龄+城市`联合索引，整体选择性可达 0.95），即使单个字段选择性低，只要整体满足标准，收益仍大于开销。
						- 联合索引的选择性计算：
							- 方式一（MySQL5.7及以下）：`联合区分度 = COUNT(DISTINCT CONCAT(字段1, 分隔符, 字段2, ..., 字段n)) / COUNT(*)`，但是存在以下**问题**：
								1. 字段含分隔符时（即使提前规避，仍可能因业务变更遗漏）导致计算失真；
								2. 字符串拼接会触发隐式类型转换（如数值型字段转字符串），且大数据量表中 `CONCAT` 函数会消耗额外 CPU 资源，效率远低于官方原生多列 `DISTINCT`。
							- 方式二（MySQL5.7以上）：`SELECT COUNT(DISTINCT col1, col2)/COUNT(*) FROM 表`。通过以下方法提高**大表**的计算效率：
								- 通过 `INFORMATION_SCHEMA.TABLES` 获取总记录数（`TABLE_ROWS`）
								- 结合 `COUNT(DISTINCT 多列)` 抽样计算（如取 10% 数据抽样，误差控制在 5% 以内）
					- 普通索引：
						- 普通索引的选择性计算：`区分度 = COUNT(DISTINCT colunmn) / COUNT(*)`
						- 选择性 ≥ 0.8（80%）：高选择性索引，查询时能大幅减少无效数据扫描，收益远大于开销（优先建索引）；
						- 0.3（30%）＜ 选择性 ＜ 0.8（80%）：中选择性索引，需结合 “**查询 / 写入频率比**” 进一步判断（如查询极频繁，仍可建索引）；
						- 选择性 ≤ 0.3（30%）：低选择性索引（如性别、状态字段，仅 2~3 个取值），**单独建索引几乎无收益**）。
							- 当单字段索引选择性**低于 0.2** 时，**MySQL 优化器**会认为索引扫描的开销（遍历索引页 + 可能的回表）高于全表扫描，**从而放弃使用索引，直接走全表扫描**，反而降低性能。
							- 例外：
								- 搭配联合查询：
									- 作为**前导**字段：低区分度前导字段可快速缩小索引扫描范围，后续高区分度字段进一步精准筛选，最终联合索引的区分度≥80%（高区分度），远优于单字段索引。
									- **高频**范围查询：低区分度字段的范围查询虽扫描条目多，但索引能避免**全表扫描**的 “逐行遍历聚簇索引” 开销，尤其当范围条件对应的索引页已缓存到缓冲池时，效率提升显著。
								- 低区分度字段 + 覆盖索引（避免回表开销）：低区分度字段的覆盖索引虽需扫描较多索引条目，但无回表 IO 开销（回表 IO 是 InnoDB 查询的核心瓶颈），总开销仍低于 “全表扫描 + 回表”。
								- 数据倾斜场景（低区分度字段存在 “高选择性子集”）：即使字段整体区分度低，但若**查询的特定取值**对应的**索引条目极少**，优化器仍会选择走索引，避免全表扫描。
				- 关键平衡：平衡查询收益与写入开销
					- 比值 ≥ 5:1：查询频率远高于写入频率（如用户信息表、商品信息表）
						- 建索引的查询收益完全覆盖写入开销，建议建索引；
					- 3:1 ＜ 比值 ＜ 5:1：查询频率高于写入频率（如订单表）
						- 若查询是核心业务（如订单查询），仍建议建索引；
						- 若写入更核心（如订单实时生成），需要进一步优化索引（如建立覆盖索引）
							- 覆盖索引：收益更高，量化标准可放宽：覆盖索引无需回表，查询收益大幅提升，且写入开销与普通索引一致，此时：
							    - 索引选择性标准可放宽至 0.6（60%）；
							    - 查询 / 写入频率比可放宽至 3:1。
					- 比值 ≤ 3:1：写入频率接近或高于查询频率（如日志表、实时采集表）
						- 建索引的写入开销会超过查询收益，**不建议建索引**（除非是极核心的低频查询，可考虑定时建临时索引）。
						- 特殊补充：若写入操作以 “**批量插入**” 为主（如数据同步、批量导入
							- MyISAM 引擎
								- 写入时可临时关闭索引（`ALTER TABLE 表名 DISABLE KEYS;`），导入后再开启（`ALTER TABLE 表名 ENABLE KEYS;`），此时写入开销可大幅降低，比值标准可**放宽至 2:1**。
									- **设计初衷**：临时禁用非唯一二级索引的维护，以提升批量插入 / 更新数据的效率
									- 执行 `ALTER TABLE ... ENABLE KEYS` 时，MyISAM 会**批量重建所有被禁用的非唯一二级索引**，批量重建的效率远高于逐条维护索引的效率（这也是该命令的核心价值）
							- InnoDB 引擎
								- InnoDB 若需提升批量导入效率，可以
									1. 先删除二级索引，
									2. 然后使用 `LOAD DATA INFILE` 、
									3. 最后重建索引
								- 但是建 / 删 / 重建都会导致使用表锁，从而导致线上业务阻塞
									- 解决措施：使用 **[[Online DDL]]**，实现线上无感知索引操作
				- 辅助量化：数据量阈值
					- 通用阈值：表的总行数 ＜ 1 万行（或数据量 ＜ 100MB），建索引的收益＜开销，建议不建索引；
					- 原理：小表全表扫描可在内存中快速完成（毫秒级），索引查询反而需要额外的 B + 树遍历开销，且索引会占用额外磁盘空间。
		- **内存缓存效率**：决定过滤的「执行速度」，减少磁盘 IO 等待
			- 内存缓存效率的衡量维度：
				- 缓存容量利用率：固定大小的缓冲池，能缓存的索引节点数量多少；
				- 缓存命中率：查询需要的索引节点，恰好被缓存到缓冲池（内存）中的概率高低。缓存容量利用率越高、命中率越高，内存缓存效率就越强。
			- 原理：索引过滤的过程，本质是 “在索引节点中匹配目标值、排除无关值” 的过程，而这个过程的执行介质（内存 / 磁盘），直接决定了执行速度：内存IO的效率是磁盘IO的十万倍以上
				- 缓存容量利用率高：**索引条目越短**，单个索引节点占用的内存越小，固定大小的缓冲池能缓存更多的索引节点，意味着更多的索引过滤操作能在内存中完成，无需触发磁盘 IO；
				- 缓存命中率高：**缓存的索引节点越多**，查询时需要的索引节点恰好被缓存的概率就越高，能直接在内存中快速完成索引匹配和数据过滤，无需等待磁盘页的读取加载，过滤速度大幅提升。
			- 量化标准：
				- **缓冲池命中率**≥95%：若命中率 < 95%，即使索引选择性高，也会因频繁读磁盘导致过滤效率下降
				- **索引页**占比 40%~70%：
					- 索引页占比过低（<30%）：说明大量索引页**未被缓存**，过滤时需频繁读磁盘；
					- 索引页占比过高（>80%）：**挤压数据页缓存**，导致回表时数据页需读磁盘，反而降低整体效率
				- **脏页率**≤30%：脏页率过高：InnoDB 会优先**刷新脏页**，导致缓存淘汰机制失衡，**有用的索引页可能被淘汰**，**间接**降低索引过滤的**缓存命中率**
				- **淘汰频率**≤1000 次 / 秒（高并发）：淘汰频率过高：说明**缓冲池空间不足**，或**索引页过大 / 过多**，导致常用的索引页被频繁替换，过滤时需重复读磁盘，效率下降
			- 启示：
				- 要降低索引条目长度（[[降低索引长字段负面影响的核心方法]]）
				- 要适当提高缓存池大小
	2. 排序效率：能否避免文件排序（Using filesort）
		- **判断标准：排序字段是否利用索引有序性**
			- **单列排序**：排序字段需在索引中，且无范围查询破坏有序性；
			- **联合排序**：遵循「索引排序顺序与查询排序顺序一致」（正序/倒序可通过反向遍历索引实现，混合正倒序无法利用索引）；
			- **边界**：范围查询后的字段仍可用于索引排序（如`(a,b,c)`，`WHERE a=1 AND b BETWEEN 2 AND 3 ORDER BY c`可利用索引排序），但无法用于过滤。
			- **工具验证**：`EXPLAIN`的`Extra`字段无「Using filesort」（低频排序允许例外，需权衡收益）。
	3. 回表开销：权衡是否值得为了[[覆盖索引]]，而往联合索引增加列

- 维度2：成本平衡维度（收益＞成本，可持续性）
	1. 维护开销：索引过多会增加优化器选择索引的耗时，且可能产生冗余索引，增加运维成本。
	 2. 写入成本： 写入开销（核心）：INSERT/UPDATE/DELETE 时，需同步维护索引 B + 树（分裂、合并、更新节点），增加写入耗时；
		- 核心逻辑：索引维护成本与「索引数量」「索引长度」「写操作频率」正相关；
		- 量化判断：
			- 写多读少场景（如实时日志表，写:读＞1:5）：优先精简索引（单列索引/短联合索引），避免覆盖索引/长联合索引（写操作延迟增幅≥50%）；
			- 写少读多场景（如商品详情表，读:写＞10:1）：可接受较高维护成本，优先选择覆盖索引/联合索引（查询收益远大于维护成本）；
			- 具体开销：每新增1个索引，INSERT/UPDATE/DELETE耗时增加5%-10%；索引字段越长，页分裂概率越高，维护开销越大。
	 3. 存储成本：是否在合理范围
		- 量化阈值：索引总大小 / 表数据大小 ≤ 150%（超过该值需精简低价值索引）；
		- 优化方向：通过长字段优化（如前缀索引、字段轻量化）减少索引体积，避免索引体积超过表数据体积导致的存储浪费。



